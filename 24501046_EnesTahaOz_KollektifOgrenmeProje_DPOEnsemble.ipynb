{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a7TXwwwTykQ0"
      },
      "outputs": [],
      "source": [
        "!pip install trl\n",
        "!pip install bitsandbytes\n",
        "!pip install llm-blender\n",
        "!git clone https://github.com/arcee-ai/mergekit.git\n",
        "%cd mergekit\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5l3dZcXx2M5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset, concatenate_datasets,load_dataset\n",
        "from peft import LoraConfig, TaskType, PeftModel\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXqBxC5PvVT0"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "che_ykwLxNtn"
      },
      "outputs": [],
      "source": [
        "CHECKPOINTS_PHASE_1_BASE_DIR = \"/content/drive/MyDrive/KO_Project_2/checkpoints_phase_1\"\n",
        "CHECKPOINTS_PHASE_2_BASE_DIR = \"/content/drive/MyDrive/KO_Project_2/checkpoints_phase_2\"\n",
        "MODELS_PHASE_1_SAVE_BASE_DIR = \"/content/drive/MyDrive/KO_Project_2/models_phase_1\"\n",
        "MODELS_PHASE_2_SAVE_BASE_DIR = \"/content/drive/MyDrive/KO_Project_2/models_phase_2\"\n",
        "LOGS_PHASE_1_BASE_DIR = \"/content/drive/MyDrive/KO_Project_2/logs_phase_1\"\n",
        "LOGS_PHASE_2_BASE_DIR = \"/content/drive/MyDrive/KO_Project_2/logs_phase_2\"\n",
        "PLOTS_PHASE_1_BASE_DIR = \"/content/drive/MyDrive/KO_Project_2/plots_phase_1\"\n",
        "PLOTS_PHASE_2_BASE_DIR = \"/content/drive/MyDrive/KO_Project_2/plots_phase_2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D73Z4PBiD_Cv"
      },
      "outputs": [],
      "source": [
        "DATASET = load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdTl4OBskayt"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if TOKENIZER.pad_token is None:\n",
        "    TOKENIZER.pad_token = TOKENIZER.eos_token\n",
        "\n",
        "TOKENIZER.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrzBQvF8hah7"
      },
      "outputs": [],
      "source": [
        "# Veri setini prompt uzunluğu, prompt + seçilmiş uzunluğu ve prompt + reddedilmiş uzunluğu üzerine filtrelenmesi üzerine kullanılır.\n",
        "def helper_filter_dpo_dataset(example):\n",
        "\n",
        "    chosen_messages = example['chosen']\n",
        "    rejected_messages = example['rejected']\n",
        "\n",
        "    prompt_messages = chosen_messages[:-1]\n",
        "\n",
        "    prompt_len = len(TOKENIZER.apply_chat_template(prompt_messages, tokenize=True))\n",
        "\n",
        "    chosen_full_len = len(TOKENIZER.apply_chat_template(chosen_messages, tokenize=True))\n",
        "\n",
        "    rejected_full_len = len(TOKENIZER.apply_chat_template(rejected_messages, tokenize=True))\n",
        "\n",
        "    if prompt_len > 256:\n",
        "        return False\n",
        "    if chosen_full_len > 512:\n",
        "        return False\n",
        "    if rejected_full_len > 512:\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKPHgQyZiEac"
      },
      "outputs": [],
      "source": [
        "# Veri setinin filtrelenmesini üstteki fonksiyonu da kullanarak tamamlar.\n",
        "def helper_apply_filtering(dataset, name=\"Dataset\"):\n",
        "    original_count = len(dataset)\n",
        "\n",
        "    filtered_dataset = dataset.filter(helper_filter_dpo_dataset, num_proc=os.cpu_count())\n",
        "\n",
        "    new_count = len(filtered_dataset)\n",
        "    removed_count = original_count - new_count\n",
        "\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(f\"Orijinal: {original_count}\")\n",
        "    print(f\"Kalan   : {new_count}\")\n",
        "    print(f\"Atılan  : {removed_count} (Oran: {removed_count/original_count:.2%})\\n\")\n",
        "\n",
        "    return filtered_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnNHjCLLiXhq"
      },
      "outputs": [],
      "source": [
        "DATASET = helper_apply_filtering(DATASET[\"train\"], name=\"Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSUy8_Sligez"
      },
      "outputs": [],
      "source": [
        "DATASET_SELECTED_SUBSETS_LIST = [\"sharegpt\", \"evol_instruct\", \"ultrachat\", \"flan_v2_niv2\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlgUemkUl9LA"
      },
      "outputs": [],
      "source": [
        "# Veri kümesinden alt kümeleri elde etmek amacıyla bu fonksiyon kullanılır.\n",
        "def helper_get_dataset_subset(subset_name, subset_size):\n",
        "  subset_ds = DATASET.filter(lambda x: x[\"source\"] == subset_name)\n",
        "\n",
        "  subset_ds = subset_ds.shuffle(42)\n",
        "\n",
        "  if len(subset_ds) >= subset_size:\n",
        "    subset_ds = subset_ds.select(range(subset_size))\n",
        "    print(f\"{subset_name} alt kümesi için {subset_size} adet veri seçildi\")\n",
        "  else:\n",
        "    subset_ds = subset_ds.select(range(len(subset_ds)))\n",
        "    print(f\"{subset_name} alt kümesi için {len(subset_ds)} adet veri seçildi. {subset_size} kadar veri yok.\")\n",
        "  return subset_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjmPkdfUo2H6"
      },
      "outputs": [],
      "source": [
        "# Her bir alt küme için Eğitim, Validasyon ve Test kümeleri elde etmek amacıyla kullanılır.\n",
        "def helper_get_subset_train_test_validation(subset_ds, subset_name):\n",
        "  train_test_and_valid = subset_ds.train_test_split(test_size=0.2, seed=42, shuffle=True)\n",
        "  test_and_valid = train_test_and_valid[\"test\"].train_test_split(test_size=0.5, seed=42, shuffle=True)\n",
        "\n",
        "  train = train_test_and_valid['train']\n",
        "  validation = test_and_valid['train']\n",
        "  test = test_and_valid['test']\n",
        "\n",
        "  print(f\"---{subset_name} alt kümesi için---\")\n",
        "  print(f\"Train sayısı     : {len(train)}\")\n",
        "  print(f\"Validation sayısı: {len(validation)}\")\n",
        "  print(f\"Test sayısı      : {len(test)}\\n\")\n",
        "\n",
        "  return train, validation, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxkT2hxFnwkN"
      },
      "outputs": [],
      "source": [
        "SUBSET_DATASETS_LIST = [helper_get_dataset_subset(subset_name, 2500) for subset_name in DATASET_SELECTED_SUBSETS_LIST]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Yjko940qqq8"
      },
      "outputs": [],
      "source": [
        "SUBSET_1_TRAIN, SUBSET_1_VAL, SUBSET_1_TEST = helper_get_subset_train_test_validation(SUBSET_DATASETS_LIST[0], \"sharegpt\")\n",
        "SUBSET_2_TRAIN, SUBSET_2_VAL, SUBSET_2_TEST = helper_get_subset_train_test_validation(SUBSET_DATASETS_LIST[1], \"evol_instruct\")\n",
        "SUBSET_3_TRAIN, SUBSET_3_VAL, SUBSET_3_TEST = helper_get_subset_train_test_validation(SUBSET_DATASETS_LIST[2], \"ultrachat\")\n",
        "SUBSET_4_TRAIN, SUBSET_4_VAL, SUBSET_4_TEST= helper_get_subset_train_test_validation(SUBSET_DATASETS_LIST[3], \"flan_v2_niv2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxuM5YzwqDQM"
      },
      "outputs": [],
      "source": [
        "TOTAL_TRAIN = concatenate_datasets([SUBSET_1_TRAIN, SUBSET_2_TRAIN, SUBSET_3_TRAIN, SUBSET_4_TRAIN]).shuffle(42)\n",
        "TOTAL_VAL = concatenate_datasets([SUBSET_1_VAL, SUBSET_2_VAL, SUBSET_3_VAL, SUBSET_4_VAL]).shuffle(42)\n",
        "TOTAL_TEST = concatenate_datasets([SUBSET_1_TEST, SUBSET_2_TEST, SUBSET_3_TEST, SUBSET_4_TEST]).shuffle(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgNCJtGvg5LH"
      },
      "outputs": [],
      "source": [
        "# Müfredat Öğrenmesine uygun olacak biçimde daha önceden elde edilmiş alt kümeler \"kolay\" ve \"zor\" olarak ikiye ayrılır.\n",
        "def helper_prepare_curriculum_subset_data(subset_train, split_ratio=0.5):\n",
        "\n",
        "    subset_train_with_margin = subset_train.map(lambda x: {\"margin\": x[\"chosen-rating\"] - x[\"rejected-rating\"]})\n",
        "\n",
        "    subset_train_with_margin_df = subset_train_with_margin.to_pandas()\n",
        "\n",
        "    subset_train_with_margin_df_sorted = subset_train_with_margin_df.sort_values(by=\"margin\", ascending=False)\n",
        "\n",
        "    split_index = int(len(subset_train_with_margin_df_sorted) * split_ratio)\n",
        "\n",
        "    subset_train_easy_df = subset_train_with_margin_df_sorted.iloc[:split_index]\n",
        "\n",
        "    subset_train_hard_df = subset_train_with_margin_df_sorted.iloc[split_index:]\n",
        "\n",
        "    print(f\"Kolay Örnek Sayısı (Phase 1): {len(subset_train_easy_df)} (Ort. Margin: {subset_train_easy_df['margin'].mean():.2f})\")\n",
        "    print(f\"Zor Örnek Sayısı (Phase 2): {len(subset_train_hard_df)} (Ort. Margin: {subset_train_hard_df['margin'].mean():.2f}\\n\")\n",
        "\n",
        "    subset_train_easy = Dataset.from_pandas(subset_train_easy_df)\n",
        "    subset_train_hard = Dataset.from_pandas(subset_train_hard_df)\n",
        "\n",
        "    return subset_train_easy, subset_train_hard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-0D2bBtu_UU"
      },
      "outputs": [],
      "source": [
        "SUBSET_1_TRAIN_EASY, SUBSET_1_TRAIN_HARD = helper_prepare_curriculum_subset_data(SUBSET_1_TRAIN, split_ratio=0.5)\n",
        "SUBSET_2_TRAIN_EASY, SUBSET_2_TRAIN_HARD = helper_prepare_curriculum_subset_data(SUBSET_2_TRAIN, split_ratio=0.5)\n",
        "SUBSET_3_TRAIN_EASY, SUBSET_3_TRAIN_HARD = helper_prepare_curriculum_subset_data(SUBSET_3_TRAIN, split_ratio=0.5)\n",
        "SUBSET_4_TRAIN_EASY, SUBSET_4_TRAIN_HARD = helper_prepare_curriculum_subset_data(SUBSET_4_TRAIN, split_ratio=0.5)\n",
        "TOTAL_TRAIN_EASY, TOTAL_TRAIN_HARD = helper_prepare_curriculum_subset_data(TOTAL_TRAIN, split_ratio=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "67A0S9RT7oXW"
      },
      "outputs": [],
      "source": [
        "# Eğitim sürecinde kullanılacak global değerler ayarlanmıştır. PEFT_CONFIG = Modellerin Faz 1 ve Faz 2 DPO eğitimleri sürecinde eğitilen LoRA katmanlarının config'ini temsil eder.\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "IS_SUPPORT_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "IS_SUPPORT_FP16 = torch.cuda.is_available() and not IS_SUPPORT_BF16\n",
        "\n",
        "\n",
        "PEFT_CONFIG = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThiL53ux9ttA"
      },
      "outputs": [],
      "source": [
        "# Modellerin Faz 1 ve Faz 2 DPO eğitimi sürecinde kullanılacak DPO ayarlamalarını fazlara göre şekillenecek biçimde üretilmesini sağlar.\n",
        "def helper_generate_dpo_training_args(dir, phase_select):\n",
        "  return DPOConfig(\n",
        "    output_dir= dir,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "\n",
        "    report_to=\"tensorboard\",\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    beta=0.1 if phase_select==1 else 0.08,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-6 if phase_select==1 else 3e-6,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    bf16=IS_SUPPORT_BF16,\n",
        "    fp16=IS_SUPPORT_FP16,\n",
        "    remove_unused_columns=True,\n",
        "    max_length=512,\n",
        "    max_prompt_length=256,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=30,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=30,\n",
        "    per_device_eval_batch_size=2,\n",
        "    eval_accumulation_steps=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_rewards/margins\",\n",
        "    greater_is_better=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCtMrkzDP2gz"
      },
      "outputs": [],
      "source": [
        "# Baz model oluşturulmasını sağlar.\n",
        "def helper_create_base_model(model_id = MODEL_ID):\n",
        "  base_model = AutoModelForCausalLM.from_pretrained(model_id, dtype=DTYPE, use_cache=False).to(DEVICE)\n",
        "  return base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXO052B39_Hc"
      },
      "outputs": [],
      "source": [
        "# Verilen DPO ayarlaması, eğitim ve validasyon setleri ve model için eğitim fazına göre değişiklik gösterecek şekilde eğiticiyi oluşturur.\n",
        "def helper_generate_dpo_trainer(training_args, training_dataset, val_dataset, phase_select, model=None):\n",
        "\n",
        "  if model is None and phase_select == 1:\n",
        "    base_model = helper_create_base_model()\n",
        "  elif phase_select == 2:\n",
        "    base_model = model\n",
        "  else:\n",
        "    print(\"Fonksiyon kullanımında hata tespit edildi.\")\n",
        "\n",
        "  base_model.enable_input_require_grads()\n",
        "  base_model.config.use_cache = False\n",
        "\n",
        "  return DPOTrainer(\n",
        "    model=base_model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    train_dataset=training_dataset,\n",
        "    processing_class=TOKENIZER,\n",
        "    peft_config=PEFT_CONFIG if phase_select==1 else None,\n",
        "    eval_dataset=val_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUa0En7A_NHX"
      },
      "outputs": [],
      "source": [
        "# Eğitim sonucu elde edilen LoRA adapterların kaydedilmesini sağlar.\n",
        "def helper_save_adapter(trainer, dir):\n",
        "  trainer.save_model(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGOF0dK2_e7h"
      },
      "outputs": [],
      "source": [
        "# Kaydedilen LoRa adapterların geri yüklenmesini sağlar.\n",
        "def helper_load_model(dir):\n",
        "  base_model = helper_create_base_model()\n",
        "  adapter_path = dir\n",
        "  return PeftModel.from_pretrained(base_model, adapter_path, is_trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwDcZLAWoXV6"
      },
      "outputs": [],
      "source": [
        "# Verilen model üzerinden tek bir prompt için çıktı üretimini gerçekleştirir.\n",
        "@torch.inference_mode()\n",
        "def helper_generate_output_for_single_prompt(raw_prompt, model_and_tokenizer_path):\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_and_tokenizer_path, dtype=DTYPE).to(DEVICE)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_and_tokenizer_path)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  messages = [{\"content\": raw_prompt, \"role\": \"user\"}]\n",
        "\n",
        "  formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "  tokenized_inputs = tokenizer(\n",
        "        formatted_prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        add_special_tokens=False\n",
        "    ).to(DEVICE)\n",
        "\n",
        "  input_ids_tensor = tokenized_inputs[\"input_ids\"]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      **tokenized_inputs,\n",
        "      max_new_tokens=256,\n",
        "      do_sample=True,\n",
        "      temperature=0.7,\n",
        "      top_p=0.95,\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "  )\n",
        "\n",
        "\n",
        "  response = outputs[0][input_ids_tensor.shape[-1]:]\n",
        "  decoded_response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "  del model\n",
        "  del tokenizer\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "\n",
        "  return decoded_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLc77etv1Rww"
      },
      "outputs": [],
      "source": [
        "# Verilen model üzerinden, verilen birden fazla prompt için batch mantığıyla çıktıların üretimi gerçekleştirilir.\n",
        "@torch.inference_mode()\n",
        "def helper_generate_output_for_many_prompts_with_batches(model_and_tokenizer_path, prompts, batch_size=8, device=DEVICE):\n",
        "\n",
        "    model =  AutoModelForCausalLM.from_pretrained(model_and_tokenizer_path, dtype=DTYPE).to(device)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_and_tokenizer_path)\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_outputs = []\n",
        "    all_processed_prompts = []\n",
        "\n",
        "    print(f\"{len(prompts)} tane prompt işleniyor.\")\n",
        "\n",
        "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
        "        batch_raw_prompts = prompts[i : i + batch_size]\n",
        "\n",
        "        formatted_batch = []\n",
        "\n",
        "        for raw_prompt in batch_raw_prompts:\n",
        "            messages = [{\"content\": raw_prompt, \"role\": \"user\"}]\n",
        "            formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            formatted_batch.append(formatted_prompt)\n",
        "\n",
        "        tokenized_inputs = tokenizer(\n",
        "            formatted_batch,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            add_special_tokens=False\n",
        "        ).to(device)\n",
        "\n",
        "        input_ids_tensor = tokenized_inputs[\"input_ids\"]\n",
        "        input_length = input_ids_tensor.shape[-1]\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **tokenized_inputs,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        generated_tokens = outputs[:, input_length:]\n",
        "        decoded_batch = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        all_outputs.extend(decoded_batch)\n",
        "        all_processed_prompts.extend(batch_raw_prompts)\n",
        "\n",
        "    data_dict = {\n",
        "        \"prompt\": all_processed_prompts,\n",
        "        \"responses\": all_outputs\n",
        "    }\n",
        "\n",
        "    dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "    del model\n",
        "    del tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLfN5isDnwxr"
      },
      "outputs": [],
      "source": [
        "# Eğitim sonunda elde edilen logların kaydedilmesini sağlar.\n",
        "def helper_save_train_logs(log_history, dir):\n",
        "  folder_path = os.path.dirname(dir)\n",
        "\n",
        "  if folder_path and not os.path.exists(folder_path):\n",
        "      os.makedirs(folder_path, exist_ok=True)\n",
        "      print(f\"Klasör oluşturuldu: {folder_path}\")\n",
        "\n",
        "  with open(dir, \"w\") as f:\n",
        "    json.dump(log_history, f, indent=4)\n",
        "\n",
        "  print(f\"Loglar {dir} konumuna kaydedildi\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB49QMXioRY2"
      },
      "outputs": [],
      "source": [
        "# Kaydedilen eğitim loglarının geri yüklenmesini gerçekleştirir.\n",
        "def helper_load_train_logs(dir):\n",
        "  with open(dir,\"r\") as f:\n",
        "    log = json.load(f)\n",
        "    print(f\"Loglar {dir} konumundan yüklendi\\n\")\n",
        "  return log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGsZJ5a9iuv8"
      },
      "outputs": [],
      "source": [
        "# Faz 1 ve Faz 2 DPO eğitimine dair süreçlerin eğitim loglarını kullanarak görselleştirilmesini sağlar ve görselleştirmelerin kaydedilmesini gerçekleştirir.\n",
        "def helper_plot_dpo_metrics(log_history, phase_select, trainer_select, dir):\n",
        "\n",
        "    train_related_data = []\n",
        "    eval_related_data = []\n",
        "\n",
        "    for entry in log_history:\n",
        "        if \"loss\" in entry and \"eval_loss\" not in entry:\n",
        "            train_related_data.append({\n",
        "                \"step\": entry[\"step\"],\n",
        "                \"loss\": entry[\"loss\"],\n",
        "                \"accuracy\": entry.get(\"rewards/accuracies\"),\n",
        "                \"margin\": entry.get(\"rewards/margins\"),\n",
        "                \"chosen\": entry.get(\"rewards/chosen\"),\n",
        "                \"rejected\": entry.get(\"rewards/rejected\"),\n",
        "                \"type\": \"Train\"\n",
        "            })\n",
        "\n",
        "        if \"eval_loss\" in entry:\n",
        "            eval_related_data.append({\n",
        "                \"step\": entry[\"step\"],\n",
        "                \"loss\": entry[\"eval_loss\"],\n",
        "                \"accuracy\": entry.get(\"eval_rewards/accuracies\"),\n",
        "                \"margin\": entry.get(\"eval_rewards/margins\"),\n",
        "                \"chosen\": entry.get(\"eval_rewards/chosen\"),\n",
        "                \"rejected\": entry.get(\"eval_rewards/rejected\"),\n",
        "                \"type\": \"Eval\"\n",
        "            })\n",
        "\n",
        "    if not train_related_data and not eval_related_data:\n",
        "        print(f\"Phase {phase_select} için veri bulunamadı.\")\n",
        "        return\n",
        "\n",
        "    df_train_related_data = pd.DataFrame(train_related_data)\n",
        "    df_eval_related_data = pd.DataFrame(eval_related_data)\n",
        "\n",
        "\n",
        "    sns.set_theme(style=\"darkgrid\")\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "    fig.suptitle(f'DPO Eğitim Analizi (Train vs Eval) - Trainer {trainer_select} Phase {phase_select}', fontsize=16, fontweight='bold')\n",
        "\n",
        "    ax = axes[0, 0]\n",
        "    if not df_train_related_data.empty:\n",
        "        sns.lineplot(ax=ax, data=df_train_related_data, x=\"step\", y=\"loss\", label=\"Train Loss\", color=\"tab:blue\", linestyle=\"-\")\n",
        "    if not df_eval_related_data.empty:\n",
        "        sns.lineplot(ax=ax, data=df_eval_related_data, x=\"step\", y=\"loss\", label=\"Eval Loss\", color=\"tab:orange\", linestyle=\"--\", linewidth=2)\n",
        "    ax.set_title(\"Training and Eval Loss\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.legend()\n",
        "\n",
        "    ax = axes[0, 1]\n",
        "    if not df_train_related_data.empty and \"accuracy\" in df_train_related_data.columns and df_train_related_data[\"accuracy\"].notna().any():\n",
        "        sns.lineplot(ax=ax, data=df_train_related_data, x=\"step\", y=\"accuracy\", label=\"Train Acc\", color=\"green\", linestyle=\"-\", alpha=0.6, marker=\"s\")\n",
        "    if not df_eval_related_data.empty and \"accuracy\" in df_eval_related_data.columns and df_eval_related_data[\"accuracy\"].notna().any():\n",
        "        sns.lineplot(ax=ax, data=df_eval_related_data, x=\"step\", y=\"accuracy\", label=\"Eval Acc\", color=\"darkgreen\", linestyle=\"--\", linewidth=2, marker=\"o\")\n",
        "\n",
        "    ax.set_title(\"Training and Eval Accuracies\")\n",
        "    ax.set_ylabel(\"Accuracy\")\n",
        "    ax.axhline(0.5, ls=':', color='gray', alpha=0.5)\n",
        "    ax.legend()\n",
        "\n",
        "    ax = axes[1, 1]\n",
        "    if not df_train_related_data.empty and \"margin\" in df_train_related_data.columns and df_train_related_data[\"margin\"].notna().any():\n",
        "        sns.lineplot(ax=ax, data=df_train_related_data, x=\"step\", y=\"margin\", label=\"Train Margin\", color=\"purple\", linestyle=\"-\", alpha=0.6, marker=\"s\")\n",
        "    if not df_eval_related_data.empty and \"margin\" in df_eval_related_data.columns and df_eval_related_data[\"margin\"].notna().any():\n",
        "        sns.lineplot(ax=ax, data=df_eval_related_data, x=\"step\", y=\"margin\", label=\"Eval Margin\", color=\"indigo\", linestyle=\"--\", linewidth=2, marker=\"o\")\n",
        "\n",
        "    ax.set_title(\"Train and Eval Margin\")\n",
        "    ax.set_ylabel(\"Margin Score\")\n",
        "    ax.legend()\n",
        "\n",
        "    ax = axes[1, 0]\n",
        "\n",
        "    if not df_train_related_data.empty and \"chosen\" in df_train_related_data.columns and df_train_related_data[\"chosen\"].notna().any():\n",
        "        sns.lineplot(ax=ax, data=df_train_related_data, x=\"step\", y=\"chosen\", color=\"blue\", linestyle=\"-\", alpha=0.4, label=\"Train Chosen\")\n",
        "        sns.lineplot(ax=ax, data=df_train_related_data, x=\"step\", y=\"rejected\", color=\"red\", linestyle=\"-\", alpha=0.4, label=\"Train Rejected\")\n",
        "\n",
        "    if not df_eval_related_data.empty and \"chosen\" in df_eval_related_data.columns and df_eval_related_data[\"chosen\"].notna().any():\n",
        "        sns.lineplot(ax=ax, data=df_eval_related_data, x=\"step\", y=\"chosen\", color=\"blue\", linestyle=\"--\", linewidth=2, label=\"Eval Chosen\")\n",
        "        sns.lineplot(ax=ax, data=df_eval_related_data, x=\"step\", y=\"rejected\", color=\"red\", linestyle=\"--\", linewidth=2, label=\"Eval Rejected\")\n",
        "\n",
        "    ax.set_title(\"Train and Eval Rewards\")\n",
        "    ax.set_ylabel(\"Rewards\")\n",
        "    ax.legend(fontsize='small')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    folder_path = os.path.dirname(dir)\n",
        "\n",
        "    if folder_path and not os.path.exists(folder_path):\n",
        "      os.makedirs(folder_path, exist_ok=True)\n",
        "      print(f\"Klasör oluşturuldu: {folder_path}\")\n",
        "\n",
        "    if dir:\n",
        "        plt.savefig(dir, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Plot '{dir}' konumuna kaydedildi.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgLcOhLBfp7h"
      },
      "outputs": [],
      "source": [
        "all_training_args_phase_1 = [\n",
        "    helper_generate_dpo_training_args(f\"{CHECKPOINTS_PHASE_1_BASE_DIR}/dpo_adapter_{i}\", phase_select=1)\n",
        "    for i in range(1, 6)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qa-qAFpvIJXn"
      },
      "outputs": [],
      "source": [
        "phase_1_configs = [\n",
        "    (1, all_training_args_phase_1[0], SUBSET_1_TRAIN_EASY, SUBSET_1_VAL),\n",
        "    (2, all_training_args_phase_1[1], SUBSET_2_TRAIN_EASY, SUBSET_2_VAL),\n",
        "    (3, all_training_args_phase_1[2], SUBSET_3_TRAIN_EASY, SUBSET_3_VAL),\n",
        "    (4, all_training_args_phase_1[3], SUBSET_4_TRAIN_EASY, SUBSET_4_VAL),\n",
        "    (5, all_training_args_phase_1[4], TOTAL_TRAIN_EASY, TOTAL_VAL),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMZG7tGZfp7h"
      },
      "outputs": [],
      "source": [
        "# 4 ayrı alt küme için oluşturulmuş modellerin ve birleştirilmiş veri seti ile eğitilecek modelin Faz 1 DPO Eğitimi gerçekleştirilir. Loglar kaydedilir.\n",
        "for model_id, args, train_ds, val_ds in phase_1_configs:\n",
        "\n",
        "    print(f\"\\n{'='*10} Model {model_id} için Phase 1 Eğitimi Başlıyor {'='*10}\")\n",
        "\n",
        "    trainer = helper_generate_dpo_trainer(\n",
        "        training_args=args,\n",
        "        training_dataset=train_ds,\n",
        "        val_dataset=val_ds,\n",
        "        phase_select=1\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    adapter_save_path = f\"{MODELS_PHASE_1_SAVE_BASE_DIR}/dpo_adapter_{model_id}\"\n",
        "    helper_save_adapter(trainer, adapter_save_path)\n",
        "    print(f\"Adapter kaydedildi: {adapter_save_path}\")\n",
        "\n",
        "    log_history = trainer.state.log_history[:]\n",
        "    log_save_path = f\"{LOGS_PHASE_1_BASE_DIR}/log_trainer{model_id}_phase_1.json\"\n",
        "    helper_save_train_logs(log_history, log_save_path)\n",
        "    print(f\"Loglar kaydedildi: {log_save_path}\")\n",
        "\n",
        "    print(f\"Model {model_id} bellekten siliniyor.\")\n",
        "\n",
        "    del trainer\n",
        "    del log_history\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Model {model_id} tamamlandı.\\n\")\n",
        "\n",
        "print(\"Tüm modeller başarıyla eğitildi ve adapterlar kaydedildi.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if_7z0rpfp7h"
      },
      "outputs": [],
      "source": [
        "#Faz 1 Eğitim sürecini tamamlayan modellerin eğitim süreçlerine ait grafikler çizdirilip kaydedilir.\n",
        "print(\"Grafik çizdirme başlıyor.\")\n",
        "\n",
        "for i in range(1, 6):\n",
        "    log_file_path = f\"{LOGS_PHASE_1_BASE_DIR}/log_trainer{i}_phase_1.json\"\n",
        "    plot_save_path = f\"{PLOTS_PHASE_1_BASE_DIR}/plot_dpo_metrics_trainer{i}_phase_1.png\"\n",
        "\n",
        "\n",
        "    current_log = helper_load_train_logs(log_file_path)\n",
        "\n",
        "    helper_plot_dpo_metrics(\n",
        "        current_log,\n",
        "        phase_select=1,\n",
        "        trainer_select=i,\n",
        "        dir=plot_save_path\n",
        "    )\n",
        "\n",
        "    print(f\"Grafik başarıyla kaydedildi: Trainer {i} - Phase 1\")\n",
        "\n",
        "print(\"Tüm grafik işlemleri tamamlandı.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NxhB0pyfp7h"
      },
      "outputs": [],
      "source": [
        "all_training_args_phase_2 = [\n",
        "    helper_generate_dpo_training_args(f\"{CHECKPOINTS_PHASE_2_BASE_DIR}/dpo_adapter_{i}\", phase_select=2)\n",
        "    for i in range(1, 6)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UMhESA2JucW"
      },
      "outputs": [],
      "source": [
        "model_paths = [\"/dpo_adapter_1\", \"/dpo_adapter_2\" ,\"/dpo_adapter_3\" ,\"/dpo_adapter_4\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbDCcFfbfp7h"
      },
      "outputs": [],
      "source": [
        "phase_2_configs = [\n",
        "    (1, all_training_args_phase_2[0], SUBSET_1_TRAIN_HARD, SUBSET_1_VAL),\n",
        "    (2, all_training_args_phase_2[1], SUBSET_2_TRAIN_HARD, SUBSET_2_VAL),\n",
        "    (3, all_training_args_phase_2[2], SUBSET_3_TRAIN_HARD, SUBSET_3_VAL),\n",
        "    (4, all_training_args_phase_2[3],SUBSET_4_TRAIN_HARD, SUBSET_4_VAL),\n",
        "    (5, all_training_args_phase_2[4],TOTAL_TRAIN_HARD, TOTAL_VAL),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KAiFKZEfp7h"
      },
      "outputs": [],
      "source": [
        "# 4 ayrı alt küme için oluşturulmuş modellerin ve birleştirilmiş veri seti ile eğitilecek modelin Faz 2 DPO Eğitimi gerçekleştirilir. Loglar kaydedilir.\n",
        "for model_id, args, train_ds, val_ds in phase_2_configs:\n",
        "\n",
        "    print(f\"\\n{'='*10} Model {model_id} için Phase 2 Eğitimi Başlıyor {'='*10}\")\n",
        "\n",
        "    load_path = f\"{MODELS_PHASE_1_SAVE_BASE_DIR}/dpo_adapter_{model_id}\"\n",
        "    print(f\"Phase 1 Modeli yükleniyor: {load_path}\")\n",
        "\n",
        "    current_model = helper_load_model(load_path)\n",
        "\n",
        "    trainer = helper_generate_dpo_trainer(\n",
        "        model=current_model,\n",
        "        training_args=args,\n",
        "        training_dataset=train_ds,\n",
        "        val_dataset=val_ds,\n",
        "        phase_select=2\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    adapter_save_path = f\"{MODELS_PHASE_2_SAVE_BASE_DIR}/dpo_adapter_{model_id}\"\n",
        "    helper_save_adapter(trainer, adapter_save_path)\n",
        "    print(f\"Adapter kaydedildi: {adapter_save_path}\")\n",
        "\n",
        "    log_history = trainer.state.log_history[:]\n",
        "    log_save_path = f\"{LOGS_PHASE_2_BASE_DIR}/log_trainer{model_id}_phase_2.json\"\n",
        "    helper_save_train_logs(log_history, log_save_path)\n",
        "    print(f\"Loglar kaydedildi: {log_save_path}\")\n",
        "\n",
        "    print(f\"Model {model_id} bellekten temizleniyor.\")\n",
        "\n",
        "    del trainer\n",
        "    del current_model\n",
        "    del log_history\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Model {model_id} tamamlandı.\\n\")\n",
        "\n",
        "print(\"Tüm modeller başarıyla eğitildi ve adapterlar kaydedildi.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4GUGfvnfp7h"
      },
      "outputs": [],
      "source": [
        "#Faz 2 Eğitim sürecini tamamlayan modellerin eğitim süreçlerine ait grafikler çizdirilip kaydedilir.\n",
        "print(\"Grafik çizdirme başlıyor.\")\n",
        "\n",
        "for i in range(1, 6):\n",
        "    log_file_path = f\"{LOGS_PHASE_2_BASE_DIR}/log_trainer{i}_phase_2.json\"\n",
        "    plot_save_path = f\"{PLOTS_PHASE_2_BASE_DIR}/plot_dpo_metrics_trainer{i}_phase_2.png\"\n",
        "\n",
        "\n",
        "    current_log = helper_load_train_logs(log_file_path)\n",
        "\n",
        "    helper_plot_dpo_metrics(\n",
        "        current_log,\n",
        "        phase_select=2,\n",
        "        trainer_select=i,\n",
        "        dir=plot_save_path\n",
        "    )\n",
        "\n",
        "    print(f\"Grafik başarıyla kaydedildi: Trainer {i} Phase 2\")\n",
        "\n",
        "print(\"Tüm grafik işlemleri tamamlandı.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaXysmYFSzZc"
      },
      "outputs": [],
      "source": [
        "# Eğitim sonucu elde edilen LoRA adapterların base modelle birleştirilmesini sağlar.\n",
        "def fuse_adapters_into_models(adapter_dirs, save_base_dir, phase_select , model_id=MODEL_ID,):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    fused_model_paths = []\n",
        "\n",
        "    if not os.path.exists(save_base_dir):\n",
        "        os.makedirs(save_base_dir, exist_ok=True)\n",
        "\n",
        "    for i, adapter_path in enumerate(adapter_dirs):\n",
        "        full_model_name = f\"dpo_full_model_{i+1}_phase_{phase_select}\"\n",
        "        save_path = os.path.join(save_base_dir, full_model_name)\n",
        "\n",
        "        print(f\"Adapter, Model ile Birleştiriliyor {i+1}: {adapter_path}\")\n",
        "\n",
        "        model = helper_load_model(adapter_path)\n",
        "\n",
        "        model = model.merge_and_unload()\n",
        "\n",
        "        model.save_pretrained(save_path)\n",
        "        tokenizer.save_pretrained(save_path)\n",
        "        fused_model_paths.append(save_path)\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(f\"Model Kaydedildi: {save_path}\\n\")\n",
        "\n",
        "    return fused_model_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b45kSdt9TR4d"
      },
      "outputs": [],
      "source": [
        "# 4 modelin birleştirilmesi sürecine yönelik MergeKit config oluşturulur. Modellerin hangi ağırlıkta birleştirileceğini kesinleştirmek için Faz 2 eğitimi sonundaki performansları değerlendirilir.\n",
        "def create_merge_config(log_paths, fused_model_paths, output_yaml=\"config.yaml\", base_model=MODEL_ID):\n",
        "\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
        "        dtype_str = \"bfloat16\"\n",
        "        print(\"Donanım bf16 destekliyor. Config 'bfloat16' olarak ayarlanıyor.\")\n",
        "    else:\n",
        "        dtype_str = \"float16\"\n",
        "        print(\"Donanım bf16 desteklemiyor. Config 'float16' olarak ayarlanıyor.\")\n",
        "\n",
        "    peak_accuracies = []\n",
        "\n",
        "    for i, log_path in enumerate(log_paths):\n",
        "        log_history = helper_load_train_logs(log_path)\n",
        "\n",
        "        eval_accs = [\n",
        "            entry.get('eval_rewards/accuracies', entry.get('eval_accuracy', 0.5))\n",
        "            for entry in log_history\n",
        "            if 'eval_rewards/accuracies' in entry or 'eval_accuracy' in entry\n",
        "        ]\n",
        "\n",
        "        peak_acc = max(eval_accs) if eval_accs else 0.5\n",
        "        peak_accuracies.append(peak_acc)\n",
        "        print(f\"Model {i+1} Peak Accuracy: {peak_acc:.4f}\")\n",
        "\n",
        "    acc_np = np.array(peak_accuracies)\n",
        "    squared_accs = acc_np ** 2\n",
        "    weights = squared_accs / np.sum(squared_accs)\n",
        "\n",
        "    yaml_lines = [\n",
        "        \"merge_method: dare_ties\",\n",
        "        f\"base_model: {base_model}\",\n",
        "        \"tokenizer_source: base\",\n",
        "        \"parameters:\",\n",
        "        \"  int8_mask: true\",\n",
        "        \"  normalize: true\",\n",
        "        f\"dtype: {dtype_str}\",\n",
        "        \"models:\"]\n",
        "\n",
        "    for path, w in zip(fused_model_paths, weights):\n",
        "        yaml_lines.append(f\"  - model: {path}\")\n",
        "        yaml_lines.append(f\"    parameters:\")\n",
        "        yaml_lines.append(f\"      weight: {w:.4f}\")\n",
        "        yaml_lines.append(f\"      density: 0.7\")\n",
        "\n",
        "    final_yaml = \"\\n\".join(yaml_lines)\n",
        "\n",
        "    with open(output_yaml, \"w\") as f:\n",
        "        f.write(final_yaml)\n",
        "\n",
        "    print(f\"\\n Merge config kaydedildi: {os.path.abspath(output_yaml)}\")\n",
        "    for i, w in enumerate(weights): print(f\" - Model {i+1}: {w:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERcW5dlW1spn"
      },
      "outputs": [],
      "source": [
        "PHASE_1_ADAPTER_DIRS = [\n",
        "    f\"{MODELS_PHASE_1_SAVE_BASE_DIR}/dpo_adapter_{i}\"\n",
        "    for i in range(1, 6)\n",
        "]\n",
        "\n",
        "PHASE_2_ADAPTER_DIRS = [\n",
        "    f\"{MODELS_PHASE_2_SAVE_BASE_DIR}/dpo_adapter_{i}\"\n",
        "    for i in range(1, 6)\n",
        "]\n",
        "\n",
        "PHASE_2_LOG_DIRS = [\n",
        "    f\"{LOGS_PHASE_2_BASE_DIR}/log_trainer{i}_phase_2.json\"\n",
        "    for i in range(1, 6)\n",
        "]\n",
        "\n",
        "\n",
        "FUSED_MODELS_SAVE_BASE_DIR = \"/content/drive/MyDrive/KO_Project_2/full_models\"\n",
        "\n",
        "CONFIG_YAML_DIR = \"/content/drive/MyDrive/KO_Project_2/config.yaml\"\n",
        "\n",
        "FUSED_MODELS_PATH_PHASE_1 = fuse_adapters_into_models(PHASE_1_ADAPTER_DIRS, FUSED_MODELS_SAVE_BASE_DIR, phase_select=1)\n",
        "FUSED_MODELS_PATH_PHASE_2 = fuse_adapters_into_models(PHASE_2_ADAPTER_DIRS, FUSED_MODELS_SAVE_BASE_DIR, phase_select=2)\n",
        "\n",
        "\n",
        "create_merge_config(PHASE_2_LOG_DIRS[:4], FUSED_MODELS_PATH_PHASE_2[:4], output_yaml=CONFIG_YAML_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpdUor6o18K7"
      },
      "outputs": [],
      "source": [
        "# Merge işlemi gerçekleştirilir.\n",
        "!mergekit-yaml /content/drive/MyDrive/KO_Project_2/config.yaml /content/drive/MyDrive/KO_Project_2/merged_model_final \\\n",
        "    --allow-crimes \\\n",
        "    --copy-tokenizer \\\n",
        "    --out-shard-size 1B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz2Nqeu8fp7p"
      },
      "outputs": [],
      "source": [
        "MERGED_MODEL_FINAL_PATH = \"/content/drive/MyDrive/KO_Project_2/merged_model_final\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUdYGjuA6FVN"
      },
      "outputs": [],
      "source": [
        "# Verilen model ve test seti için DPO metriklerinin test edilmesi sağlanır.\n",
        "def evaluate_model_on_DPO_metrics(merged_model, eval_dataset, base_model_id=MODEL_ID, beta=0.08):\n",
        "\n",
        "    print(f\"Referans Model Yükleniyor ({base_model_id}).\")\n",
        "\n",
        "    ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        torch_dtype=merged_model.dtype,\n",
        "        device_map=DEVICE\n",
        "    )\n",
        "    ref_model.eval()\n",
        "\n",
        "    eval_args = DPOConfig(\n",
        "        output_dir=\"eval_temp_dir\",\n",
        "        per_device_eval_batch_size=2,\n",
        "        remove_unused_columns=True,\n",
        "        beta=beta,\n",
        "        label_names=[],\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = DPOTrainer(\n",
        "        model=merged_model,\n",
        "        ref_model=ref_model,\n",
        "        args=eval_args,\n",
        "        train_dataset=eval_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        processing_class=TOKENIZER,\n",
        "    )\n",
        "\n",
        "    print(\"DPO Metrikleri üzerinden Test Başlıyor.\")\n",
        "\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    clean_metrics = {\n",
        "        \"Accuracy\": metrics.get(\"eval_rewards/accuracies\", metrics.get(\"eval_accuracy\", 0.0)),\n",
        "        \"Margin\": metrics.get(\"eval_rewards/margins\", metrics.get(\"eval_margins\", 0.0)),\n",
        "        \"Loss\": metrics.get(\"eval_loss\", 0.0)\n",
        "    }\n",
        "\n",
        "    print(\"Test Tamamlandı.\")\n",
        "\n",
        "    del ref_model\n",
        "    del trainer\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return clean_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp7dpXqHfp7q"
      },
      "outputs": [],
      "source": [
        "# Test sonuçlarının görselleştirilip kaydedilmesini sağlar.\n",
        "def plot_model_comparisons(results_dict, save_path=\"model_comparison.png\", dpi=300):\n",
        "\n",
        "    model_names = list(results_dict.keys())\n",
        "    metrics = [\"Accuracy\", \"Margin\", \"Loss\"]\n",
        "\n",
        "    colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
        "\n",
        "    data = {metric: [] for metric in metrics}\n",
        "    for model in model_names:\n",
        "        for metric in metrics:\n",
        "            val = results_dict[model].get(metric, 0.0)\n",
        "            data[metric].append(val)\n",
        "\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.25\n",
        "    multiplier = 0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6), layout='constrained')\n",
        "\n",
        "    for attribute, color in zip(metrics, colors):\n",
        "        offset = width * multiplier\n",
        "        rects = ax.bar(x + offset, data[attribute], width, label=attribute, color=color, alpha=0.9, edgecolor='black', linewidth=0.5)\n",
        "        ax.bar_label(rects, padding=3, fmt='%.2f', fontsize=9, rotation=0)\n",
        "        multiplier += 1\n",
        "\n",
        "    ax.set_ylabel('Değer')\n",
        "    ax.set_title('Model Performans Karşılaştırması (DPO Test Seti)', fontsize=14, pad=15, fontweight='bold')\n",
        "    ax.set_xticks(x + width, model_names)\n",
        "    ax.set_ylim(0, 1.1)\n",
        "    ax.legend(loc='upper left', ncols=3)\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "\n",
        "    save_dir = os.path.dirname(save_path)\n",
        "    if save_dir and not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    plt.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Grafik başarıyla kaydedildi: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1MDDUUWfp7q"
      },
      "outputs": [],
      "source": [
        "MODELS_TO_TEST = [\n",
        "    (\"Tek Model\", f\"{FUSED_MODELS_SAVE_BASE_DIR}/dpo_full_model_5_phase_2\"),\n",
        "    (\"Merged Model\", MERGED_MODEL_FINAL_PATH),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzowwwOYfp7q"
      },
      "outputs": [],
      "source": [
        "# Birleştirilmiş veri seti ile Faz 1 ve Faz 2 DPO eğitimini sağlamış tek model ile Merged edilmiş model test edilir.\n",
        "all_metrics_storage = {}\n",
        "\n",
        "for name, path in MODELS_TO_TEST:\n",
        "    print(f\"\\n{'='*10} Test ediliyor: {name} {'='*10}\")\n",
        "    print(f\"Yükleniyor: {path}\")\n",
        "\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        path,\n",
        "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
        "        device_map=\"cuda\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    metrics = evaluate_model_on_DPO_metrics(\n",
        "        merged_model=model,\n",
        "        eval_dataset=TOTAL_TEST,\n",
        "        base_model_id=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        beta=0.1\n",
        "    )\n",
        "\n",
        "    print(f\"Sonuçlar: {metrics}\")\n",
        "\n",
        "    all_metrics_storage[name] = metrics\n",
        "\n",
        "\n",
        "    if 'model' in locals():\n",
        "        del model\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"Test bitti, grafik çiziliyor.\")\n",
        "plot_model_comparisons(\n",
        "    results_dict=all_metrics_storage,\n",
        "    save_path=\"/content/drive/MyDrive/KO_Project_2/plots/final_benchmark.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCuMNGD6xGAB"
      },
      "outputs": [],
      "source": [
        "# Merge edilmiş modelin çıktı performansını görmek amacıyla kullanılmıştır.\n",
        "SAMPLE_SIZE = 50\n",
        "print(f\"Toplam Test Verisi: {len(TOTAL_TEST)}\")\n",
        "\n",
        "test_sample = TOTAL_TEST.shuffle(seed=42).select(range(SAMPLE_SIZE))\n",
        "\n",
        "input_prompts = test_sample[\"prompt\"]\n",
        "\n",
        "print(f\"{len(input_prompts)} adet prompt işleniyor.\")\n",
        "\n",
        "generated_dataset = helper_generate_output_for_many_prompts_with_batches(\n",
        "    model_and_tokenizer_path=MERGED_MODEL_FINAL_PATH,\n",
        "    prompts=input_prompts,\n",
        "    batch_size=8,\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "df_results = generated_dataset.to_pandas()\n",
        "\n",
        "df_results.rename(columns={\"prompt\": \"Input_Prompt\", \"responses\": \"Merged_Model_Response\"}, inplace=True)\n",
        "\n",
        "save_path_csv = \"/content/drive/MyDrive/KO_Project_2/merged_model_test_outputs.csv\"\n",
        "df_results.to_csv(save_path_csv, index=False)\n",
        "\n",
        "print(f\"\\n İşleme tamamlandı. Sonuçlar kaydedildi: {save_path_csv}\")\n",
        "\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "print(df_results.head(3))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}